{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Student Name: Ashwathy Ashokan\n",
        "Student ID: C0935859\n",
        "Subject: Big Data Framework 01\n",
        "Assignment: RDDs and Spark SQL"
      ],
      "metadata": {
        "id": "yKYvffTy4VOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section A: RDD Operations"
      ],
      "metadata": {
        "id": "iB-F_vtm5YOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: RDD Creation and Basic Operations"
      ],
      "metadata": {
        "id": "RZzPppWE5fMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RDD Assignment\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Load sales.csv\n",
        "sales_rdd = sc.textFile(\"sales.csv\")\n",
        "header = sales_rdd.first()\n",
        "sales_data = sales_rdd.filter(lambda row: row != header)\n",
        "\n",
        "# Display first 5 rows\n",
        "print(sales_data.take(5))\n",
        "\n",
        "# Count transactions\n",
        "print(f\"Total transactions: {sales_data.count()}\")\n",
        "\n",
        "# Extract (product_id, price)\n",
        "prod_price = sales_data.map(lambda x: (x.split(\",\")[1], float(x.split(\",\")[4])))\n",
        "print(prod_price.take(5))"
      ],
      "metadata": {
        "id": "XGjl7sYU5f8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Transformations and Actions"
      ],
      "metadata": {
        "id": "lA5Gmsl35jGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming columns: transaction_id, product_id, user_id, quantity, price\n",
        "\n",
        "# Total revenue\n",
        "revenue_rdd = sales_data.map(lambda x: float(x.split(\",\")[3]) * float(x.split(\",\")[4]))\n",
        "print(f\"Total revenue: {revenue_rdd.sum()}\")\n",
        "\n",
        "# Unique products sold\n",
        "product_ids = sales_data.map(lambda x: x.split(\",\")[1]).distinct()\n",
        "print(f\"Unique products: {product_ids.count()}\")\n",
        "\n",
        "# Filter where quantity > 1\n",
        "filtered = sales_data.filter(lambda x: int(x.split(\",\")[3]) > 1)\n",
        "print(filtered.take(5))\n",
        "\n",
        "# (product_id, revenue)\n",
        "prod_rev = sales_data.map(lambda x: (x.split(\",\")[1], float(x.split(\",\")[3]) * float(x.split(\",\")[4])))\n",
        "total_rev_per_prod = prod_rev.reduceByKey(lambda x, y: x + y)\n",
        "print(total_rev_per_prod.take(5))"
      ],
      "metadata": {
        "id": "J7DxYnVj5sw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Working with Multiple RDDs"
      ],
      "metadata": {
        "id": "0e0bPTgQ5xNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_rdd = sc.textFile(\"products.csv\").filter(lambda x: x != \"product_id,product_name,category\")\n",
        "users_rdd = sc.textFile(\"users.csv\").filter(lambda x: x != \"user_id,user_name,location\")\n",
        "\n",
        "# Keyed by product_id\n",
        "products_kv = products_rdd.map(lambda x: (x.split(\",\")[0], x.split(\",\")[1]))  # product_id, product_name\n",
        "sales_kv = sales_data.map(lambda x: (x.split(\",\")[1], (x.split(\",\")[0], float(x.split(\",\")[3]) * float(x.split(\",\")[4]))))\n",
        "\n",
        "# Join sales with products\n",
        "joined_sales = sales_kv.join(products_kv)\n",
        "result = joined_sales.map(lambda x: (x[1][0][0], x[1][1], x[1][0][1]))  # transaction_id, product_name, revenue\n",
        "\n",
        "# Now join with users\n",
        "users_kv = users_rdd.map(lambda x: (x.split(\",\")[0], (x.split(\",\")[1], x.split(\",\")[2])))\n",
        "sales_users_kv = sales_data.map(lambda x: (x.split(\",\")[2], (x.split(\",\")[0], float(x.split(\",\")[3]) * float(x.split(\",\")[4]))))\n",
        "\n",
        "final_join = sales_users_kv.join(users_kv).map(lambda x: (x[1][0][0], x[1][1][0], x[1][1][1], x[1][0][1]))  # transaction_id, user_name, location, revenue\n"
      ],
      "metadata": {
        "id": "DEkiy8Is5z0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: Advanced RDD Operations"
      ],
      "metadata": {
        "id": "I5oVXWcn52u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 products by revenue\n",
        "top5_products = total_rev_per_prod.takeOrdered(5, key=lambda x: -x[1])\n",
        "print(\"Top 5 products by revenue:\", top5_products)\n",
        "\n",
        "# Total spending per user\n",
        "user_spend = sales_data.map(lambda x: (x.split(\",\")[2], float(x.split(\",\")[3]) * float(x.split(\",\")[4])))\n",
        "spend_by_user = user_spend.reduceByKey(lambda x, y: x + y)\n",
        "print(spend_by_user.take(5))"
      ],
      "metadata": {
        "id": "L-Q4bWys56M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section B: DataFrames and SQL"
      ],
      "metadata": {
        "id": "Ak6Vq0e958t7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: DataFrame Creation and Exploration"
      ],
      "metadata": {
        "id": "RO4ELjI16BaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
        "products_df = spark.read.csv(\"products.csv\", header=True, inferSchema=True)\n",
        "users_df = spark.read.csv(\"users.csv\", header=True, inferSchema=True)\n",
        "\n",
        "sales_df.printSchema()\n",
        "products_df.printSchema()\n",
        "users_df.printSchema()\n",
        "\n",
        "sales_df.show(5)\n",
        "products_df.show(5)\n",
        "users_df.show(5)"
      ],
      "metadata": {
        "id": "rG2vOv9l6KE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: SQL Queries"
      ],
      "metadata": {
        "id": "XkM79OiQ6L0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.createOrReplaceTempView(\"sales\")\n",
        "products_df.createOrReplaceTempView(\"products\")\n",
        "users_df.createOrReplaceTempView(\"users\")\n",
        "\n",
        "# Total revenue\n",
        "spark.sql(\"SELECT SUM(quantity * price) AS total_revenue FROM sales\").show()\n",
        "\n",
        "# Top 5 users\n",
        "spark.sql(\"\"\"\n",
        "    SELECT u.user_name, SUM(s.quantity * s.price) AS total_spent\n",
        "    FROM sales s JOIN users u ON s.user_id = u.user_id\n",
        "    GROUP BY u.user_name\n",
        "    ORDER BY total_spent DESC\n",
        "    LIMIT 5\n",
        "\"\"\").show()\n",
        "\n",
        "# Count of products sold by category\n",
        "spark.sql(\"\"\"\n",
        "    SELECT p.category, COUNT(s.product_id) AS product_sold\n",
        "    FROM sales s JOIN products p ON s.product_id = p.product_id\n",
        "    GROUP BY p.category\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "id": "gMglyX2i6WPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: Joins and Aggregations"
      ],
      "metadata": {
        "id": "X4qAR0tA6YZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enriched_df = sales_df.join(users_df, \"user_id\").join(products_df, \"product_id\")\n",
        "enriched_df = enriched_df.withColumn(\"revenue\", enriched_df[\"quantity\"] * enriched_df[\"price\"])\n",
        "enriched_df.select(\"transaction_id\", \"user_name\", \"location\", \"product_name\", \"category\", \"quantity\", \"price\", \"revenue\").show(5)\n",
        "\n",
        "# Revenue per location\n",
        "enriched_df.groupBy(\"location\").sum(\"revenue\").show()\n",
        "\n",
        "# Avg quantity per category\n",
        "enriched_df.groupBy(\"category\").avg(\"quantity\").show()\n"
      ],
      "metadata": {
        "id": "OktMSez36bCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8: Window Functions and Ranking"
      ],
      "metadata": {
        "id": "NqUhtuOo6en7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, sum as _sum\n",
        "\n",
        "# Rank users by spending within location\n",
        "user_spend_df = enriched_df.groupBy(\"user_id\", \"user_name\", \"location\").agg(_sum(\"revenue\").alias(\"total_spent\"))\n",
        "windowSpec = Window.partitionBy(\"location\").orderBy(user_spend_df[\"total_spent\"].desc())\n",
        "ranked_users = user_spend_df.withColumn(\"rank\", rank().over(windowSpec))\n",
        "ranked_users.show()\n",
        "\n",
        "# Top product per category by revenue\n",
        "prod_rev_df = enriched_df.groupBy(\"category\", \"product_name\").agg(_sum(\"revenue\").alias(\"total_revenue\"))\n",
        "windowSpec2 = Window.partitionBy(\"category\").orderBy(prod_rev_df[\"total_revenue\"].desc())\n",
        "top_products = prod_rev_df.withColumn(\"rank\", rank().over(windowSpec2)).filter(\"rank = 1\")\n",
        "top_products.show()\n"
      ],
      "metadata": {
        "id": "PHT73z9e6huQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus: Caching and Parquet"
      ],
      "metadata": {
        "id": "WMCdouxg6jk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache enriched df\n",
        "enriched_df.cache()\n",
        "\n",
        "# Write to Parquet\n",
        "enriched_df.write.mode(\"overwrite\").parquet(\"output/enriched_data.parquet\")\n",
        "\n",
        "# Read back\n",
        "parquet_df = spark.read.parquet(\"output/enriched_data.parquet\")\n",
        "parquet_df.groupBy(\"location\").sum(\"revenue\").show()\n"
      ],
      "metadata": {
        "id": "lOQjRfk16oWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}